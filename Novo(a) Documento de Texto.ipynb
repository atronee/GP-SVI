{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions.kl import kl_divergence\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "\n",
    "def RBFKernel(input_dim, variance=1., lengthscale=1.):\n",
    "    \"\"\"\n",
    "    Takes Pytorch input_dim and returns the result of the RBF kernel function\n",
    "    \"\"\"\n",
    "    return variance * torch.exp(-0.5 * torch.cdist(input_dim / lengthscale, input_dim / lengthscale) ** 2)\n",
    "\n",
    "\n",
    "\n",
    "class SVGP():\n",
    "    def __init__(self, X, Y, Z, batchsize=None):\n",
    "        \"\"\"\n",
    "        Stochastic Variational GP.\n",
    "\n",
    "        For Gaussian Likelihoods\n",
    "\n",
    "        Based on Gaussian Processes for Big data, Hensman, Fusi and Lawrence, UAI 2013,\n",
    "\n",
    "        But with natural gradients. This is a stochastic version of the SVGP model.\n",
    "        \"\"\"\n",
    "        self.kernel =  RBFKernel\n",
    "        self.batchsize = batchsize\n",
    "        self.X_all, self.Y_all = X, Y\n",
    "        if batchsize is None:\n",
    "            X_batch, Y_batch = X, Y\n",
    "        else:\n",
    "            self.slicer = iter(self._batch_slices())\n",
    "            X_batch, Y_batch = self.new_batch()\n",
    "\n",
    "        #assume the number of latent functions is one per col of Y unless specified\n",
    "\n",
    "        self.q_u_mean = Parameter(torch.randn(Z.shape[0], Y.shape[1]))\n",
    "\n",
    "        self.mean_function = lambda x: 0\n",
    "        self.Z = Z\n",
    "\n",
    "    def _batch_slices(self):\n",
    "        for i in range(0, len(self.X_all), self.batchsize):\n",
    "            yield slice(i, i + self.batchsize)\n",
    "\n",
    "    def _grads(self, parameters):\n",
    "        \"\"\"\n",
    "        Compute the gradients of the parameters\n",
    "        \"\"\"\n",
    "        #compute the gradients of the parameters\n",
    "        grads = torch.autograd.grad(self.objective(), parameters, create_graph=True)\n",
    "        return grads\n",
    "\n",
    "\n",
    "    def fit(self, num_iter=1000, lr=0.01):\n",
    "        optimizer = torch.optim.Adam([self.q_u_mean, self.kernel.variance, self.kernel.lengthscale, self.chol_var, self.chol_covar], lr=lr)\n",
    "        for i in range(num_iter):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.objective()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Iter {i}, Loss: {loss.item()}')\n",
    "\n",
    "    def objective(self):\n",
    "        \"\"\"\n",
    "        Compute the objective function\n",
    "        \"\"\"\n",
    "        #compute the kernel matrix\n",
    "        K = self.kernel(self.Z)\n",
    "        K += torch.eye(K.shape[0]) * 1e-6\n",
    "        L = torch.cholesky(K)\n",
    "\n",
    "        #compute the mean function\n",
    "        mean = self.mean_function(self.X)\n",
    "\n",
    "        #compute the kernel matrix between the inducing points and the data\n",
    "        Kmn = self.kernel(self.Z, self.X)\n",
    "\n",
    "        #compute the kernel matrix between the data\n",
    "        Kmm = self.kernel(self.X)\n",
    "\n",
    "        #compute the mean of the latent function\n",
    "        mu = Kmn @ torch.solve(self.Y - mean, L)[0]\n",
    "\n",
    "        #compute the covariance of the latent function\n",
    "        A = torch.solve(Kmn.t(), L)[0]\n",
    "        Sigma = Kmm - A.t() @ A\n",
    "\n",
    "        #compute the likelihood\n",
    "        likelihood = self.likelihood(mu, Sigma)\n",
    "\n",
    "        #compute the KL divergence\n",
    "        q_u = MultivariateNormal(self.q_u_mean, self.q_u_covar)\n",
    "        kl = kl_divergence(q_u, MultivariateNormal(torch.zeros_like(self.q_u_mean), K))\n",
    "\n",
    "        return -likelihood.log_prob(self.Y).sum() + kl.sum()\n",
    "    \n",
    "    def likelihood(self, mu, Sigma):\n",
    "        \"\"\"\n",
    "        Compute the normal likelihood of the data given the latent function\n",
    "        \"\"\"\n",
    "        return Normal(mu, Sigma)\n",
    "    \n",
    "    def set_data(self, X, Y):\n",
    "        \"\"\"\n",
    "        Set the data without calling parameters_changed to avoid wasted computation\n",
    "        If this is called by the stochastic_grad function this will immediately update the gradients\n",
    "        \"\"\"\n",
    "        assert X.shape[1]==self.Z.shape[1]\n",
    "        self.X, self.Y = X, Y\n",
    "\n",
    "    def new_batch(self):\n",
    "        \"\"\"\n",
    "        Return a new batch of X and Y by taking a chunk of data from the complete X and Y\n",
    "        \"\"\"\n",
    "        i = next(self.slicer)\n",
    "        return self.X_all[i], self.Y_all[i]\n",
    "\n",
    "    def stochastic_grad(self, parameters):\n",
    "        self.set_data(*self.new_batch())\n",
    "        return self._grads(parameters)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
